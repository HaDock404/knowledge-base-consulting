{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger les données\n",
    "\n",
    "Avant de nettoyer les données, il faut les charger :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger un fichier CSV\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "pd.set_option('display.max_columns', None) #permet d'afficher toutes les colonnes\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi charger d'autres formats comme Excel ```(pd.read_excel())```, JSON ```(pd.read_json())``` ou SQL ```(pd.read_sql())```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprendre les données\n",
    "Avant de nettoyer, il faut examiner l'état des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher des infos générales\n",
    "print(df.info())\n",
    "\n",
    "# Résumé statistique des colonnes numériques\n",
    "print(df.describe())\n",
    "\n",
    "# Voir les valeurs uniques d'une colonne\n",
    "print(df[\"colonne\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifier les données\n",
    "\n",
    "On peut réaliser plusieurs modifications pour plus de lisibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "            'price': 'prix(million)', \n",
    "            'area': 'air(m2)',\n",
    "            'bedrooms' : 'chambres',\n",
    "            'bathrooms' : 'sdb',\n",
    "            'stories' : 'étages',\n",
    "            'mainroad' : 'route principale',\n",
    "            'guestroom' : 'chambre ami',\n",
    "            'basement' : 'sous sol',\n",
    "            'hotwaterheating' : 'chauffage au gaz',\n",
    "            'airconditioning' : 'climatisation',\n",
    "            'parking' : 'parking',\n",
    "            'prefarea' : 'résidentiel',\n",
    "            'furnishingstatus' : 'meublé',\n",
    "            }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gérer les valeurs manquantes\n",
    "Les valeurs manquantes sont un problème courant. Voici comment les détecter et les traiter :  \n",
    "\n",
    "- Détection des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())  # Compter le nombre de NaN par colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supprimer les lignes ou colonnes avec des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Supprime toutes les lignes avec au moins un NaN\n",
    "df = df.dropna(axis=1)  # Supprime les colonnes avec au moins un NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remplacer les valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].fillna(\"valeur_par_défaut\")  # Remplir avec une valeur spécifique\n",
    "df[\"colonne\"] = df[\"colonne\"].fillna(df[\"colonne\"].mean())  # Remplir avec la moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gérer les doublons\n",
    "Les doublons peuvent fausser les analyses :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des doublons\n",
    "print(df.duplicated().sum())\n",
    "# ou aussi\n",
    "df.loc[df.duplicated()]\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction des types de données\n",
    "Parfois, les données sont mal typées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir une colonne en numérique\n",
    "df[\"colonne\"] = pd.to_numeric(df[\"colonne\"], errors=\"coerce\")\n",
    "\n",
    "# Convertir une colonne en date\n",
    "df[\"date_colonne\"] = pd.to_datetime(df[\"date_colonne\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation et Normalisation des Données\n",
    "Les données peuvent être dans des unités différentes ou avoir des échelles variées. Il est souvent nécessaire de les standardiser.\n",
    "\n",
    "- Standardisation (centrage-réduction, moyenne = 0, écart-type = 1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[[\"colonne1\", \"colonne2\"]] = scaler.fit_transform(df[[\"colonne1\", \"colonne2\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalisation (valeurs entre 0 et 1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[[\"colonne1\", \"colonne2\"]] = scaler.fit_transform(df[[\"colonne1\", \"colonne2\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection et Suppression des Valeurs Aberrantes (Outliers)\n",
    "Les valeurs aberrantes peuvent fausser les analyses statistiques.\n",
    "\n",
    "- Utilisation de l'IQR (Interquartile Rang, 50 % des valeurs centrales) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df[\"colonne\"].quantile(0.25)\n",
    "Q3 = df[\"colonne\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[~((df[\"colonne\"] < (Q1 - 1.5 * IQR)) | (df[\"colonne\"] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilisation de l'écart-type (variabilité des données) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df[\"colonne\"].mean()\n",
    "std = df[\"colonne\"].std()\n",
    "\n",
    "df = df[(df[\"colonne\"] > mean - 3 * std) & (df[\"colonne\"] < mean + 3 * std)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des Données Textuelles\n",
    "Les données textuelles nécessitent souvent un prétraitement, notamment pour du NLP ou des analyses.\n",
    "\n",
    "- Suppression des espaces en trop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mise en minuscule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppression des caractères spéciaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"colonne\"] = df[\"colonne\"].apply(lambda x: re.sub(r\"[^a-zA-Z0-9 ]\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppression des stopwords (mots inutiles) avec nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "df[\"colonne\"] = df[\"colonne\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection et Correction des Fautes de Saisie\n",
    "On peut utiliser ````fuzzywuzzy```` pour corriger des noms mal écrits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "choices = [\"Paris\", \"Marseille\", \"Lyon\"]\n",
    "df[\"colonne_corrigée\"] = df[\"colonne\"].apply(lambda x: process.extractOne(x, choices)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des Variables Catégoriques\n",
    "Si une colonne contient des catégories, il faut l'encoder en numérique :\n",
    "\n",
    "- Encodage One-Hot (binaire) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"categorie\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encodage Ordinal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[[\"Bas\", \"Moyen\", \"Élevé\"]])\n",
    "df[\"colonne\"] = encoder.fit_transform(df[[\"colonne\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion et Regroupement de Données\n",
    "Parfois, il faut combiner plusieurs sources de données.\n",
    "\n",
    "- Jointure entre deux DataFrames :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df1.merge(df2, on=\"clé_commune\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concaténation verticale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agrégation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(\"colonne_catégorique\").agg({\"colonne_numérique\": \"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatisation des Nettoyages avec des Pipelines\n",
    "Pour structurer le processus, on peut utiliser ```sklearn.pipeline```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputation\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"normalisation\", StandardScaler())\n",
    "])\n",
    "\n",
    "df[[\"colonne1\", \"colonne2\"]] = pipeline.fit_transform(df[[\"colonne1\", \"colonne2\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
