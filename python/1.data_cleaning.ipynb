{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger les données\n",
    "\n",
    "Avant de nettoyer les données, il faut les charger :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger un fichier CSV\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "pd.set_option('display.max_columns', None) #permet d'afficher toutes les colonnes\n",
    "\n",
    "# Afficher les premières lignes\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi charger d'autres formats comme Excel ```(pd.read_excel())```, JSON ```(pd.read_json())``` ou SQL ```(pd.read_sql())```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprendre les données\n",
    "Avant de nettoyer, il faut examiner l'état des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher des infos générales\n",
    "print(df.info())\n",
    "\n",
    "# Résumé statistique des colonnes numériques\n",
    "print(df.describe())\n",
    "\n",
    "# Voir les valeurs uniques d'une colonne\n",
    "print(df[\"colonne\"].unique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifier les données\n",
    "\n",
    "On peut réaliser plusieurs modifications pour plus de lisibilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "            'price': 'prix(million)', \n",
    "            'area': 'air(m2)',\n",
    "            'bedrooms' : 'chambres',\n",
    "            'bathrooms' : 'sdb',\n",
    "            'stories' : 'étages',\n",
    "            'mainroad' : 'route principale',\n",
    "            'guestroom' : 'chambre ami',\n",
    "            'basement' : 'sous sol',\n",
    "            'hotwaterheating' : 'chauffage au gaz',\n",
    "            'airconditioning' : 'climatisation',\n",
    "            'parking' : 'parking',\n",
    "            'prefarea' : 'résidentiel',\n",
    "            'furnishingstatus' : 'meublé',\n",
    "            }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gérer les valeurs manquantes\n",
    "Les valeurs manquantes sont un problème courant. Voici comment les détecter et les traiter :  \n",
    "\n",
    "- Détection des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())  # Compter le nombre de NaN par colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supprimer les lignes ou colonnes avec des valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()  # Supprime toutes les lignes avec au moins un NaN\n",
    "df = df.dropna(axis=1)  # Supprime les colonnes avec au moins un NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remplacer les valeurs manquantes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].fillna(\"valeur_par_défaut\")  # Remplir avec une valeur spécifique\n",
    "df[\"colonne\"] = df[\"colonne\"].fillna(df[\"colonne\"].mean())  # Remplir avec la moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gérer les doublons\n",
    "Les doublons peuvent fausser les analyses :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Détection des doublons\n",
    "print(df.duplicated().sum())\n",
    "# ou aussi\n",
    "df.loc[df.duplicated()]\n",
    "\n",
    "# Suppression des doublons\n",
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction des types de données\n",
    "Parfois, les données sont mal typées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir une colonne en numérique\n",
    "df[\"colonne\"] = pd.to_numeric(df[\"colonne\"], errors=\"coerce\")\n",
    "\n",
    "# Convertir une colonne en date\n",
    "df[\"date_colonne\"] = pd.to_datetime(df[\"date_colonne\"], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation et Normalisation des Données\n",
    "Les données peuvent être dans des unités différentes ou avoir des échelles variées. Il est souvent nécessaire de les standardiser.\n",
    "\n",
    "- Standardisation (centrage-réduction, moyenne = 0, écart-type = 1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[[\"colonne1\", \"colonne2\"]] = scaler.fit_transform(df[[\"colonne1\", \"colonne2\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalisation (valeurs entre 0 et 1) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[[\"colonne1\", \"colonne2\"]] = scaler.fit_transform(df[[\"colonne1\", \"colonne2\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection et Suppression des Valeurs Aberrantes (Outliers)\n",
    "Les valeurs aberrantes peuvent fausser les analyses statistiques.\n",
    "\n",
    "- Utilisation de l'IQR (Interquartile Rang, 50 % des valeurs centrales) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df[\"colonne\"].quantile(0.25)\n",
    "Q3 = df[\"colonne\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "df = df[~((df[\"colonne\"] < (Q1 - 1.5 * IQR)) | (df[\"colonne\"] > (Q3 + 1.5 * IQR)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilisation de l'écart-type (variabilité des données) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df[\"colonne\"].mean()\n",
    "std = df[\"colonne\"].std()\n",
    "\n",
    "df = df[(df[\"colonne\"] > mean - 3 * std) & (df[\"colonne\"] < mean + 3 * std)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des Données Textuelles\n",
    "Les données textuelles nécessitent souvent un prétraitement, notamment pour du NLP ou des analyses.\n",
    "\n",
    "- Suppression des espaces en trop :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mise en minuscule :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"colonne\"] = df[\"colonne\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppression des caractères spéciaux :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df[\"colonne\"] = df[\"colonne\"].apply(lambda x: re.sub(r\"[^a-zA-Z0-9 ]\", \"\", x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppression des stopwords (mots inutiles) avec nltk :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"french\"))\n",
    "\n",
    "df[\"colonne\"] = df[\"colonne\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection et Correction des Fautes de Saisie\n",
    "On peut utiliser ````fuzzywuzzy```` pour corriger des noms mal écrits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "\n",
    "choices = [\"Paris\", \"Marseille\", \"Lyon\"]\n",
    "df[\"colonne_corrigée\"] = df[\"colonne\"].apply(lambda x: process.extractOne(x, choices)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodage des Variables Catégoriques\n",
    "Si une colonne contient des catégories, il faut l'encoder en numérique :\n",
    "\n",
    "- Encodage One-Hot (binaire) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=[\"categorie\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encodage Ordinal :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "encoder = OrdinalEncoder(categories=[[\"Bas\", \"Moyen\", \"Élevé\"]])\n",
    "df[\"colonne\"] = encoder.fit_transform(df[[\"colonne\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion et Regroupement de Données\n",
    "Parfois, il faut combiner plusieurs sources de données.\n",
    "\n",
    "- Jointure entre deux DataFrames :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df1.merge(df2, on=\"clé_commune\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concaténation verticale :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df1, df2], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agrégation des données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped = df.groupby(\"colonne_catégorique\").agg({\"colonne_numérique\": \"mean\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatisation des Nettoyages avec des Pipelines\n",
    "Pour structurer le processus, on peut utiliser ```sklearn.pipeline```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputation\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"normalisation\", StandardScaler())\n",
    "])\n",
    "\n",
    "df[[\"colonne1\", \"colonne2\"]] = pipeline.fit_transform(df[[\"colonne1\", \"colonne2\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gestion des données volumineuses\n",
    "Quand on travaille avec des millions de lignes, pandas peut atteindre ses limites. On utilise alors des alternatives comme Dask, Polars ou PySpark.\n",
    "\n",
    "#### Exemple avec Dask pour charger et traiter un fichier volumineux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Chargement d'un fichier CSV volumineux\n",
    "df = dd.read_csv(\"large_dataset.csv\")\n",
    "\n",
    "# Appliquer des transformations (remplacement des NaN et conversion d'un champ)\n",
    "df = df.fillna({\"colonne1\": \"Valeur par défaut\"})\n",
    "df[\"colonne2\"] = df[\"colonne2\"].astype(float)\n",
    "\n",
    "# Sauvegarde après transformation\n",
    "df.to_csv(\"cleaned_data.csv\", single_file=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Avantages de Dask : Il permet de traiter les données en parallèle et de travailler avec des jeux de données qui dépassent la mémoire RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des valeurs aberrantes et incohérentes\n",
    "Les valeurs aberrantes (outliers) peuvent fausser les analyses et les modèles. On peut les identifier via :  \n",
    "✔ Méthodes statistiques (Z-score, IQR)  \n",
    "✔ Méthodes basées sur le Machine Learning  \n",
    "\n",
    "#### Détection des outliers avec l'IQR (Interquartile Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\"salaire\": [2500, 3000, 4000, 50000, 3200, 2900, 2800, 2600]})\n",
    "\n",
    "# Calcul des quartiles\n",
    "Q1 = df[\"salaire\"].quantile(0.25)\n",
    "Q3 = df[\"salaire\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Détection des outliers (1.5 fois l'IQR en dehors de Q1 et Q3)\n",
    "outliers = df[(df[\"salaire\"] < (Q1 - 1.5 * IQR)) | (df[\"salaire\"] > (Q3 + 1.5 * IQR))]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Solution : Remplacer par la médiane ou utiliser une transformation logarithmique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection et correction des erreurs typographiques\n",
    "Les erreurs humaines (ex: \"Frnace\" au lieu de \"France\") peuvent fausser les analyses.  \n",
    "\n",
    "✔ Fuzzy Matching avec fuzzywuzzy ou thefuzz  \n",
    "✔ Correction automatique avec textdistance ou SymSpell  \n",
    "\n",
    "#### Correction des fautes avec TheFuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import process\n",
    "\n",
    "# Liste de valeurs correctes\n",
    "pays_corrects = [\"France\", \"Allemagne\", \"Espagne\", \"Italie\"]\n",
    "\n",
    "# Valeur erronée à corriger\n",
    "valeur_a_corriger = \"Frnace\"\n",
    "\n",
    "# Trouver la correspondance la plus proche\n",
    "correction = process.extractOne(valeur_a_corriger, pays_corrects)\n",
    "print(correction)  # ('France', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Application : Corriger les noms de clients, pays, villes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fusion et Dédoublonnage des Données\n",
    "Lorsqu'on intègre des données de sources multiples, on doit fusionner et supprimer les doublons.  \n",
    "\n",
    "#### Fusion de datasets sur des clés partielles\n",
    "Exemple : On fusionne deux jeux de données où les identifiants ne sont pas identiques à 100% (```merge_asof``` pour joindre sur des valeurs proches) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({\"id_client\": [101, 102, 103], \"nom\": [\"Alice\", \"Bob\", \"Charlie\"]})\n",
    "df2 = pd.DataFrame({\"id_client\": [102, 103, 104], \"achat\": [\"Ordinateur\", \"Téléphone\", \"Tablette\"]})\n",
    "\n",
    "# Fusion basée sur l'ID client\n",
    "df_merged = pd.merge(df1, df2, on=\"id_client\", how=\"outer\")\n",
    "print(df_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Problème classique : Des identifiants mal formatés peuvent empêcher la fusion → On peut normaliser les valeurs avant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dédoublonnage avancé avec record linkage\n",
    "On peut identifier des doublons malgré des erreurs typographiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import recordlinkage\n",
    "\n",
    "indexer = recordlinkage.Index()\n",
    "indexer.block(\"nom\")  # Bloque sur les valeurs exactes\n",
    "\n",
    "df_pairs = indexer.index(df1, df2)\n",
    "\n",
    "# Comparaison des champs\n",
    "compare = recordlinkage.Compare()\n",
    "compare.string(\"nom\", \"nom\", method=\"jarowinkler\", threshold=0.85)\n",
    "\n",
    "similarities = compare.compute(df_pairs, df1, df2)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Application : Identifier des doublons dans des bases clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation des formats et des unités\n",
    "Quand on intègre des données de sources multiples, on doit harmoniser les formats.\n",
    "\n",
    "#### Conversion d'unités automatiquement\n",
    "Exemple : Conversion de devises avec ```forex-python```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forex_python.converter import CurrencyRates\n",
    "\n",
    "c = CurrencyRates()\n",
    "usd_to_eur = c.convert(\"USD\", \"EUR\", 100)\n",
    "print(usd_to_eur)  # Conversion de 100 USD en EUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Application : Uniformiser les données financières."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection d'Anomalies avec des Méthodes Avancées\n",
    "Au-delà des méthodes statistiques, on peut utiliser des algorithmes ML.\n",
    "\n",
    "#### Détection avec Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "df = pd.DataFrame({\"valeur\": [10, 12, 13, 200, 15, 11, 14]})\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.1)\n",
    "df[\"anomalie\"] = iso_forest.fit_predict(df[[\"valeur\"]])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ Application : Détection de fraudes, valeurs suspectes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
