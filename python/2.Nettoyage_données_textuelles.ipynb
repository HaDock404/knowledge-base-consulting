{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tokenization et Normalisation\n",
    "\n",
    "#### üîπ Objectif :  \n",
    "Segmenter un texte en unit√©s (mots, phrases) et le normaliser pour uniformiser l‚Äôanalyse.\n",
    "\n",
    "#### üîπ Techniques :\n",
    "- Tokenization : D√©coupage du texte en mots ou phrases.\n",
    "- Normalisation : Conversion en minuscules, suppression des caract√®res sp√©ciaux, accents, ponctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üîπ Outils :\n",
    "\n",
    "- üìå NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "texte = \"Bonjour ! Comment √ßa va ?\"\n",
    "tokens = word_tokenize(texte)\n",
    "print(tokens)  # ['Bonjour', '!', 'Comment', '√ßa', 'va', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìå spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "texte = \"Bonjour ! Comment √ßa va ?\"\n",
    "tokens = [token.text for token in nlp(texte)]\n",
    "print(tokens)  # ['Bonjour', '!', 'Comment', '√ßa', 'va', '?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìå Regex pour normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texte_normalise = re.sub(r\"[^\\w\\s]\", \"\", texte.lower())  # Suppression ponctuation et passage en minuscules\n",
    "print(texte_normalise)  # \"bonjour comment √ßa va\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stemming et Lemmatization\n",
    "#### üîπ Objectif :\n",
    "R√©duire les mots √† leur racine (stemming) ou leur forme canonique (lemmatisation).\n",
    "\n",
    "#### üîπ Diff√©rence :\n",
    "- Stemming : Coupe le mot (ex. \"mangeons\" ‚Üí \"mange\") sans forc√©ment respecter la grammaire.\n",
    "- Lemmatization : Trouve la forme correcte du mot (ex. \"mangeons\" ‚Üí \"manger\").\n",
    "#### üîπ Outils :\n",
    "- üìå Stemming avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "print(stemmer.stem(\"mangeons\"))  # \"mange\"\n",
    "print(stemmer.stem(\"√©tudiantes\"))  # \"√©tudi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìå Lemmatization avec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Les √©tudiants apprennent la programmation.\")\n",
    "lemmas = [token.lemma_ for token in doc]\n",
    "print(lemmas)  # ['le', '√©tudiant', 'apprendre', 'le', 'programmation', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Suppression des Stopwords\n",
    "#### üîπ Objectif :\n",
    "- √âliminer les mots qui n'apportent pas de valeur s√©mantique (\"le\", \"de\", \"et\", etc.).\n",
    "\n",
    "### üîπ Outils :\n",
    "- üìå Avec NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('french'))\n",
    "tokens_filtr√©s = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(tokens_filtr√©s)  # Garde les mots importants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìå Avec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtr√©s = [token.text for token in doc if not token.is_stop]\n",
    "print(tokens_filtr√©s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. D√©tection et Correction des Fautes d‚ÄôOrthographe\n",
    "#### üîπ Objectif :\n",
    "- Corriger les erreurs typographiques pour am√©liorer la recherche.\n",
    "\n",
    "#### üîπ Outils :\n",
    "- üìå Avec fuzzywuzzy (Matching approximatif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "mots_connus = [\"bonjour\", \"comment\", \"√ßa\", \"va\", \"programmation\"]\n",
    "mot_corrig√© = process.extractOne(\"prpgrammation\", mots_connus)\n",
    "print(mot_corrig√©)  # ('programmation', 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- üìå Avec textdistance (Distances de Levenshtein, Jaccard, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textdistance\n",
    "print(textdistance.levenshtein.normalized_similarity(\"programmation\", \"prpgrammation\"))  # 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extraction d‚ÄôEntit√©s Nomm√©es (NER)\n",
    "#### üîπ Objectif :\n",
    "- Identifier les entit√©s importantes (personnes, lieux, organisations).\n",
    "\n",
    "#### üîπ Outils :\n",
    "- üìå Avec spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Emmanuel Macron est le pr√©sident de la France.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "# \"Emmanuel Macron\" -> PERSON\n",
    "# \"France\" -> GPE (lieu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìå Techniques Avanc√©es de Pr√©traitement NLP pour la Recherche\n",
    "- 1. Vectorisation avanc√©e (TF-IDF, Word2Vec, FastText, BERT) pour am√©liorer la recherche s√©mantique.\n",
    "- 2. Expansion de requ√™tes : Trouver des synonymes pour enrichir les recherches.\n",
    "- 3. Correction orthographique automatique en se basant sur les mots les plus fr√©quents dans le corpus.\n",
    "- 4. Indexation efficace dans Elasticsearch avec des champs analys√©s et non analys√©s.\n",
    "- 5. NLP bas√© sur des mod√®les transformers (spaCy, Hugging Face) pour une recherche plus intelligente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
